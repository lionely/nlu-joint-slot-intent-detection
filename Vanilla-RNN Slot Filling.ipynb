{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current TODOs\n",
    "* Figure out why even though the loss can get to a magnitude of 0.01 the F-Score for slots is still bad\n",
    "* it was an issue with not cutting off the prediction after the <EOS> token is outputted. But ask Shilad about the batch size issue which affects the scoring with topk(). I think if its more than one batch pytorch picks the last element of the batch.\n",
    "    \n",
    "*** Issue is simply because when batching we get [batch , seq len] in one seq the smallest number should be <eos> but when batch size is >1 we have multiple batches together so value for <eos> occurs later than normal messing up evaluation. To get correct evaluation use batch_size =1 ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed/fold_train.json', 'r') as f:\n",
    "    atis_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flattens from two-dimensional list to one-dimensional list\"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sequences(data, length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Fixes the input and output sequences length, adding padding or truncating if necessary\n",
    "    :param data json file containing entries from atis dataset.\n",
    "    :param length the fixed length of the sentence.\n",
    "    \"\"\"\n",
    "    for sample in data['data']:\n",
    "        # adjust the sequence of input words\n",
    "        if len(sample['words']) < length:\n",
    "            # add <EOS> and <PAD> if sentence is shorter than maximum length\n",
    "            sample['words'].append('<EOS>')\n",
    "            while len(sample['words']) < length:\n",
    "                sample['words'].append('<PAD>')\n",
    "        else:\n",
    "            # otherwise truncate and add <EOS> at last position\n",
    "            sample['words'] = sample['words'][:length]\n",
    "            sample['words'][-1] = '<EOS>'\n",
    "\n",
    "        # adjust in the same way the sequence of output slots\n",
    "        if len(sample['slots']) < length:\n",
    "            sample['slots'].append('<EOS>')\n",
    "            while len(sample['slots']) < length:\n",
    "                sample['slots'].append('<PAD>')\n",
    "        else:\n",
    "            sample['slots'] = sample['slots'][:length]\n",
    "            sample['slots'][-1] = '<EOS>'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabularies(train_data):\n",
    "    \"\"\"\n",
    "    Collect the input vocabulary, the slot vocabulary and the intent vocabulary\n",
    "    :param train_data the training data containing words,slots and intent.\n",
    "    \"\"\"\n",
    "    # from a list of training examples, get three lists (columns)\n",
    "    data = train_data['data']\n",
    "    seq_in = [sample['words'] for sample in data]\n",
    "    vocab = flatten(seq_in)\n",
    "    # removing duplicated but keeping the order\n",
    "    v = ['<PAD>', '<SOS>', '<EOS>'] + vocab\n",
    "    vocab = sorted(set(v), key=lambda x: v.index(x)) # https://docs.python.org/3.3/howto/sorting.html\n",
    "    s = ['<PAD>','<EOS>'] + train_data['meta']['slot_types']\n",
    "    slot_tag = sorted(set(s), key=lambda x: s.index(x))\n",
    "    i = train_data['meta']['intent_types']\n",
    "    intent_tag = sorted(set(i), key=lambda x: i.index(x))\n",
    "\n",
    "    return vocab, slot_tag, intent_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_atis = adjust_sequences(atis_json)#padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_vocab,atis_slots,atis_intents = get_vocabularies(adjusted_atis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 122, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(atis_vocab),len(atis_slots),len(atis_intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next we map the data and set it up for Pytorch\n",
    "Remember each vocabulary for each sentence, slot and intent will have different embeddings. They are different sized vectors. The Network will try to figure out a mapping from these different vector spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mappings(vocab,forward_map,intent=False):\n",
    "    \"\"\"\n",
    "    This function takes the words in the vocabulary and creates a unique mapping to a number.\n",
    "    :param vocab contains all the words in the corpus.\n",
    "    :param forward_map a dictionary that will be populated with mappings.\n",
    "    returns populated forward_map\n",
    "    \"\"\"\n",
    "    for sample in vocab:\n",
    "        if not intent and sample not in forward_map.keys():\n",
    "            forward_map[sample]= len(forward_map)\n",
    "        else:\n",
    "            forward_map[sample]= len(forward_map)#+1# this so we can use 1 loss function\n",
    "            \n",
    "    return forward_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "word2index = {'<PAD>': 0, '<UNK>':1,'<SOS>':2,'<EOS>':3}\n",
    "create_mappings(atis_vocab,word2index)\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index = {'<PAD>' : 0,'<UNK>':1,'<EOS>':2}\n",
    "create_mappings(atis_slots,tag2index)\n",
    "index2tag = {v:k for k,v in tag2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent2index={}\n",
    "create_mappings(atis_intents,intent2index,intent=True)\n",
    "index2intent = {v:k for k,v in intent2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)\n",
    "len(intent2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we create a Tensor where each row is a mapped/embedded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq_data, mapping,map_type):\n",
    "    \"\"\"\n",
    "    :param seq a sequnce which will be embedded as a vector\n",
    "    :param mapping, a dictionary which contains how each element in the seq will be mapped to a number.\n",
    "    :param map_type 'words','slots' or 'intent'\n",
    "    returns a Pytorch Tensor.\n",
    "    \"\"\"\n",
    "    if map_type=='intent':\n",
    "        intent = seq_data[map_type]\n",
    "        embeddings = mapping[intent] if intent in mapping.keys() else -1 #mapping[\"<UNK>\"]\n",
    "        return torch.tensor(embeddings)   \n",
    "    else:\n",
    "        embed_fnc = lambda word: mapping[word] if word in mapping.keys() else mapping['<UNK>']\n",
    "        embeddings = list(map(embed_fnc, seq_data[map_type])) \n",
    "        return torch.LongTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(padded_atis):\n",
    "    \"\"\"\n",
    "    :param padded_atis, this is padded sequence data.\n",
    "           Of the form seq,slots,intent. This function coverts \n",
    "           these into tensors.\n",
    "    return train_data; [(seq_tensor,slot_tensor,intent_tensor)]\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    atis_data = padded_atis['data']\n",
    "    for i in range(len(atis_data)):\n",
    "        seq_tensor = prepare_sequence(atis_data[i],word2index,'words')\n",
    "        slot_tensor = prepare_sequence(atis_data[i],tag2index,'slots')\n",
    "        intent_tensor = prepare_sequence(atis_data[i],intent2index,'intent')\n",
    "        train_data.append((seq_tensor,slot_tensor,intent_tensor))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_training_set(adjusted_atis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_batch(batch):\n",
    "#     print(len(batch), len(batch[0]))\n",
    "    #     print([ex[0] for ex in batch])\n",
    "    seqs = torch.stack([ex[0] for ex in batch])\n",
    "    slots = torch.stack([ex[1] for ex in batch])\n",
    "    intents = torch.stack([ex[2] for ex in batch])\n",
    "   \n",
    "    return seqs,slots,intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size, train_data):\n",
    "    \"\"\"\n",
    "    Returns iteratively a batch of specified size on the data. \n",
    "    The last batch can be smaller if the total size is not multiple of the batch\n",
    "    \"\"\"\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while sindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex] #list of batch_size num of tuples.\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        #print('returning', len(batch), 'samples')\n",
    "        yield concatenate_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Joint-RNN Model\n",
    "---Ignore---\n",
    "\n",
    "Will use encoder-decoer model because of:\n",
    "Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" Advances in neural information processing systems. 2014.\n",
    "\n",
    "i) one for the input sequence and another for the output sequence, because doing\n",
    "so increases the number model parameters at negligible computational cost and makes it natural to\n",
    "train the LSTMon multiple language pairs simultaneously \n",
    "\n",
    "ii) deep LSTMs significantly outperformed shallow LSTMs, so we chose an LSTM with four layers. \n",
    "\n",
    "iii) valuable to reverse the order of the words of the input sentence. \n",
    "\n",
    "---Ignore---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An encoder/decoder that\n",
    "    takes a batch of sequences embeds the sequence and\n",
    "    then runs it through a fully connected layer to predict slots and intent.\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim,hidden_dim,emb_dim,slot_dim,intent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "       \n",
    "        self.slot_dim = slot_dim\n",
    "        self.intent_dim = intent_dim\n",
    "       \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim,batch_first=True)\n",
    "        \n",
    "        self.slot_fc = nn.Linear(hidden_dim,slot_dim)\n",
    "        self.intent_fc = nn.Linear(hidden_dim,intent_dim)\n",
    "        \n",
    "    def forward(self,src):\n",
    "        #src [seq len,batch size] -> [seq len, batch size, emb_dim]\n",
    "        embedded_seq = self.embedding(src)\n",
    "#         print('embedded_seq', embedded_seq.size())\n",
    "        outputs, hidden = self.rnn(embedded_seq)\n",
    "#         print('hidden size is ', hidden.size())\n",
    "#         print('outputs size is ', outputs.size())\n",
    "        outputs = outputs.contiguous().view(-1,self.emb_dim)\n",
    "        #print('out size',outputs.size())\n",
    "        slot_space = self.slot_fc(outputs)\n",
    "        slot_scores = slot_space\n",
    "#         print(slot_scores[0,:])\n",
    "#         slot_scores = F.softmax(slot_space, dim=1)\n",
    "#         print('after softmax',print(slot_scores))\n",
    "        #print(slot_scores.size())\n",
    "        intent_space = self.intent_fc(hidden)\n",
    "        intent_scores = intent_space\n",
    "#         print('intent_space size is ', intent_space.size())\n",
    "        #print(intent_space.size())\n",
    "#         intent_scores = F.softmax(intent_space, dim=1)\n",
    "        #print(intent_scores.size())\n",
    "        return slot_scores, intent_scores, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing models...\")\n",
    "n_layers = 1\n",
    "input_size = len(word2index)\n",
    "slot_size = len(tag2index)\n",
    "intent_size = len(intent2index)\n",
    "embed_size = 5\n",
    "hidden_size = 5\n",
    "output_size = MAX_LENGTH\n",
    "encoder = Encoder(input_size, embed_size,hidden_size,slot_size,intent_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in get_batches(256,train_data[:]):\n",
    "    inputs = batch[0]\n",
    "    inputs = inputs.to(device)\n",
    "    #slots = encoder(inputs)\n",
    "    slots, intents,_ = encoder(inputs)\n",
    "    #print(torch.argmax(slots,dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "input_size = len(word2index)\n",
    "slot_size = len(tag2index)\n",
    "intent_size = len(intent2index)\n",
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "output_size = MAX_LENGTH\n",
    "encoder = Encoder(input_size, embed_size,hidden_size,slot_size,intent_size).to(device)\n",
    "#-----------------------\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0,reduction='elementwise_mean')#ignore <pad> remember this ignoring 0 in intent as well, need 2 loss\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "#batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot Loss after epoch 0: 0.2921254336833954\n",
      "Intent Loss after epoch 0: 1.223901629447937\n",
      "-----------------------------------------------\n",
      "Slot Loss after epoch 50: 0.0007367925718426704\n",
      "Intent Loss after epoch 50: 1.2573015689849854\n",
      "-----------------------------------------------\n",
      "Time elapsed: 6.0191 mins \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    inp = get_batches(256,train_data)\n",
    "    encoder.train()\n",
    "    for data in inp:\n",
    "        # get the inputs\n",
    "        inputs, labels, intents = data[0],data[1],data[2]\n",
    "        inputs = inputs.type(torch.LongTensor).to(device)\n",
    "        labels = labels.type(torch.LongTensor).view(-1).to(device) #squashing\n",
    "        intents = intents.to(device)#.type(torch.FloatTensor)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        #out_slots = encoder(inputs)\n",
    "        out_slots, out_intents,_ = encoder(inputs)\n",
    "        slot_loss = criterion(out_slots, labels)\n",
    "        \n",
    "        out_intents = out_intents.view(-1,intent_size)\n",
    "        intent_loss = criterion(out_intents, intents) \n",
    "\n",
    "       # slot_loss.backward()        \n",
    "        slot_loss.backward(retain_graph=True)\n",
    "        intent_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch%50==0:\n",
    "        print('Slot Loss after epoch '+str(epoch)+':',slot_loss.item())\n",
    "        print('Intent Loss after epoch '+str(epoch)+':',intent_loss.item())\n",
    "        print('-----------------------------------------------')\n",
    "end = time.time()\n",
    "elapsed = (end-start)/60.\n",
    "print('Time elapsed: %.4f mins ' % (elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effects of double softmax?\n",
    "# for batch in get_batches(1,train_data[:1]):\n",
    "#     inputs = batch[0]\n",
    "#     labels = batch[1]\n",
    "#     labels = labels.to(device)\n",
    "#     inputs = inputs.to(device)\n",
    "#     slots = encoder(inputs)\n",
    "# s_scores = torch.argmax(F.softmax(slots, dim=1),dim=1)\n",
    "# print(slots ,labels,\n",
    "#      s_scores )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed/fold_test.json', 'r') as f:\n",
    "    atis_test_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_atis_test = adjust_sequences(atis_test_json)#padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_training_set(adjusted_atis_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude pad \n",
    "def calc_bat_fscores(y_pred,y_true,predict_type='slot'):\n",
    "    \"\"\"\n",
    "    Calc average f score for a batch.\n",
    "    compares each predicted output in a batch to actual output.\n",
    "    Then averages that.\n",
    "    \"\"\"\n",
    "    batch_avg_f = []\n",
    "    keep_slots = [i for i in tag2index.values() if i > 2]\n",
    "    if predict_type=='slot':\n",
    "#         for pred_i in range(len(y_pred)):# range batch size\n",
    "            #print(y_pred[pred_i].size())\n",
    "        f_score = f1_score(y_true[:len(y_pred)], y_pred,labels=keep_slots, average ='micro')  \n",
    "        batch_avg_f.append(f_score)\n",
    "    else:\n",
    "        return f1_score(y_true,y_pred,average ='micro')\n",
    "    return np.mean(batch_avg_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')# get a lot of warnings because some labels are not predicted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_argmax(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e4998183a75d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(slots.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     slots ,intents, slot_embedding = encoder(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0my_pred_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#print(y_pred_slots.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mf_slot_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_bat_fscores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_slots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(input, dim, keepdim)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_argmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_argmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _argmax(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "inp = get_batches(128,train_data)\n",
    "encoder.eval()\n",
    "f_slot_scores = []\n",
    "f_intent_scores = []\n",
    "for data in inp:\n",
    "    # get the inputs\n",
    "    inputs, labels , true_intents = data[0],data[1],data[2]\n",
    "    inputs = inputs.type(torch.LongTensor).to(device)\n",
    "    labels = labels.type(torch.LongTensor).view(-1).to(device) #squashing\n",
    "    true_intents = true_intents.to(device)\n",
    "    slots = encoder(inputs)\n",
    "    #print(slots.size())\n",
    "#     slots ,intents, slot_embedding = encoder(inputs)\n",
    "    y_pred_slots = torch.argmax(slots,dim=1)\n",
    "    #print(y_pred_slots.size())\n",
    "    f_slot_scores.append(calc_bat_fscores(y_pred_slots,labels.view(-1)))\n",
    "   #print(intents.size())\n",
    "#     y_pred_intents = torch.argmax(intents,dim=2)\n",
    "#     f_intent_scores.append(calc_bat_fscores(y_pred_intents[0],true_intents,predict_type='intent'))\n",
    "print('Mean Slot F Metric :',np.mean(f_slot_scores))\n",
    "print('-----------------------------------------------')\n",
    "# print('Mean Intent F Metric :',np.mean(f_intent_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = get_batches(1,train_data[:1])\n",
    "for data in inp:\n",
    "    # get the inputs\n",
    "    inputs, labels , true_intents = data[0],data[1],data[2]\n",
    "    inputs = inputs.type(torch.LongTensor).to(device)\n",
    "    labels = labels.type(torch.LongTensor).view(-1).to(device) #squashing\n",
    "    true_intents = true_intents.to(device)\n",
    "    slots = encoder(inputs)\n",
    "    y_pred_slots = torch.argmax(slots,dim=1)\n",
    "    print(y_pred_slots,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Slot F Metric : 0.995068033835993\n",
      "-----------------------------------------------\n",
      "Mean Intent F Metric : 0.714\n"
     ]
    }
   ],
   "source": [
    "inp = get_batches(1,test_data)\n",
    "encoder.eval()\n",
    "f_slot_scores = []\n",
    "f_intent_scores = []\n",
    "for data in inp:\n",
    "    # get the inputs\n",
    "    inputs, labels , true_intents = data[0],data[1],data[2]\n",
    "    inputs = inputs.type(torch.LongTensor).to(device)\n",
    "    labels = labels.type(torch.LongTensor).view(-1).to(device) #squashing\n",
    "    true_intents = true_intents.to(device)\n",
    "    \n",
    "    enc_output ,intents, slot_embedding = encoder(inputs)\n",
    "    #enc_output = encoder(inputs)\n",
    "    y_pred_slots = torch.argmax(enc_output,dim=1)\n",
    "    eos_idx = y_pred_slots.topk(1,largest=False)[1].item()\n",
    "    eos_cut_slots = y_pred_slots[:eos_idx]\n",
    "    f_slot_scores.append(calc_bat_fscores(eos_cut_slots,labels.view(-1)))\n",
    "    \n",
    "    y_pred_intents = torch.argmax(intents,dim=2)\n",
    "    f_intent_scores.append(calc_bat_fscores(y_pred_intents[0],true_intents,predict_type='intent'))\n",
    "#    print(eos_idx,y_pred_slots)\n",
    "print('Mean Slot F Metric :',np.mean(f_slot_scores))\n",
    "print('-----------------------------------------------')  \n",
    "print('Mean Intent F Metric :',np.mean(f_intent_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.topk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
