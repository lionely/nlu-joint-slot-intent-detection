{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.manifold import TSNE\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretty plot\n",
    "#sns.set_style(\"darkgrid\")\n",
    "a4_dims = (11.7,11.7)\n",
    "sns.set_context(\"paper\", font_scale = 1.5, rc={\"lines.linewidth\":2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data In\n",
    "We are using the Air Travel Information System (ATIS) dataset(I believe it is originally from UPenn, need to verify)\n",
    "It uses the Inside-Out-Beginning tag format(IOB) described here : https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n",
    "\n",
    "The typical structure is:\n",
    "\n",
    "Sentence (tokens), Slots to match each token, Intent.\n",
    "\n",
    "Found a preprocessed atis file from, thanks to: https://github.com/D2KLab/botcycle/tree/master/nlu/data/atis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed/fold_train.json', 'r') as f:\n",
    "    atis_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atis_json['data'][3130] #total 4978\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3130, 46)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = {}\n",
    "slot_counts = {}\n",
    "intent_counts = {}\n",
    "max_len_w_idx = [(0,atis_json['data'][0]['length'])]\n",
    "num_tokens = 0\n",
    "idx = 0\n",
    "for atis_entry in atis_json['data']:\n",
    "    num_tokens+=atis_entry['length']\n",
    "    if atis_entry['length'] > max_len_w_idx[0][1]:\n",
    "        max_len_w_idx[0] = (idx,atis_entry['length'])\n",
    "    if atis_entry['intent'] not in intent_counts.keys():\n",
    "            intent_counts[atis_entry['intent']] = 1\n",
    "    else:\n",
    "        intent_counts[atis_entry['intent']]+= 1\n",
    "    for slot in atis_entry['slots']:\n",
    "        if slot not in slot_counts.keys():\n",
    "            slot_counts[slot] = 1\n",
    "        else:\n",
    "            slot_counts[slot]+= 1\n",
    "    for word in atis_entry['words']:\n",
    "        if word not in word_counts.keys():\n",
    "            word_counts[word] = 1\n",
    "        else:\n",
    "            word_counts[word]+= 1\n",
    "    idx+=1\n",
    "    \n",
    "slot_counts\n",
    "intent_counts\n",
    "max_len_w_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3883\n",
       "Name: to, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stats_df = pd.DataFrame(word_counts,index=[0])\n",
    "word_stats_df.T.head()\n",
    "word_stats_df.T.columns\n",
    "word_stats_df.T.loc[word_stats_df.T[0].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>3883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fly</th>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>3343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "i      659\n",
       "want   148\n",
       "to    3883\n",
       "fly    250\n",
       "from  3343"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stats_df.T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Slots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_stats_df = pd.DataFrame(slot_counts,index=[0])\n",
    "labels = ['O'] #this is useless and many exist in all queries need not include.\n",
    "slot_stats_df = slot_stats_df.drop(['O'],axis=1)\n",
    "slot_stats_df.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/31029560/plotting-categorical-data-with-pandas-and-matplotlib\n",
    "new_dims = (23,15)\n",
    "fig, ax = plt.subplots(figsize=new_dims)\n",
    "slot_stats_df.T[0].plot(kind='bar')\n",
    "fig = ax.get_figure()\n",
    "plt.title('Distribution of slots in ATIS')\n",
    "fig.savefig(\"graphs/slot_dist.png\",bbox_inches=\"tight\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_stats_df = pd.DataFrame(intent_counts,index=[0])\n",
    "labels = ['O'] #this is useless and many exist in all queries need not include.\n",
    "intent_stats_df = intent_stats_df#.drop(['O'],axis=1)\n",
    "intent_stats_df.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "intent_stats_df.T[0].plot(kind='bar')\n",
    "fig = ax.get_figure()\n",
    "plt.title('Distribution of intents in ATIS')\n",
    "fig.savefig(\"graphs/intent_dist.png\",bbox_inches=\"tight\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flattens from two-dimensional list to one-dimensional list\"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sequences(data, length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Fixes the input and output sequences length, adding padding or truncating if necessary\n",
    "    :param data json file containing entries from atis dataset.\n",
    "    :param length the fixed length of the sentence.\n",
    "    \"\"\"\n",
    "    for sample in data['data']:\n",
    "        # adjust the sequence of input words\n",
    "        if len(sample['words']) < length:\n",
    "            # add <EOS> and <PAD> if sentence is shorter than maximum length\n",
    "            sample['words'].append('<EOS>')\n",
    "            while len(sample['words']) < length:\n",
    "                sample['words'].append('<PAD>')\n",
    "        else:\n",
    "            # otherwise truncate and add <EOS> at last position\n",
    "            sample['words'] = sample['words'][:length]\n",
    "            sample['words'][-1] = '<EOS>'\n",
    "\n",
    "        # adjust in the same way the sequence of output slots\n",
    "        if len(sample['slots']) < length:\n",
    "            sample['slots'].append('<EOS>')\n",
    "            while len(sample['slots']) < length:\n",
    "                sample['slots'].append('<PAD>')\n",
    "        else:\n",
    "            sample['slots'] = sample['slots'][:length]\n",
    "            sample['slots'][-1] = '<EOS>'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabularies(train_data):\n",
    "    \"\"\"\n",
    "    Collect the input vocabulary, the slot vocabulary and the intent vocabulary\n",
    "    :param train_data the training data containing words,slots and intent.\n",
    "    \"\"\"\n",
    "    # from a list of training examples, get three lists (columns)\n",
    "    data = train_data['data']\n",
    "    seq_in = [sample['words'] for sample in data]\n",
    "    vocab = flatten(seq_in)\n",
    "    # removing duplicated but keeping the order\n",
    "    v = ['<PAD>', '<SOS>', '<EOS>'] + vocab\n",
    "    vocab = sorted(set(v), key=lambda x: v.index(x)) # https://docs.python.org/3.3/howto/sorting.html\n",
    "    s = ['<PAD>', '<SOS>', '<EOS>'] + train_data['meta']['slot_types']\n",
    "    slot_tag = sorted(set(s), key=lambda x: s.index(x))\n",
    "    i = train_data['meta']['intent_types']\n",
    "    intent_tag = sorted(set(i), key=lambda x: i.index(x))\n",
    "\n",
    "    return vocab, slot_tag, intent_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_atis = adjust_sequences(atis_json)#padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_vocab,atis_slots,atis_intents = get_vocabularies(adjusted_atis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atis_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(atis_vocab),len(atis_slots),len(atis_intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next we need to embed the data and set it up for Pytorch\n",
    "Remember each vocabulary for each sentence, slot and intent will have different embeddings. They are different sized vectors. The Network will try to figure out a mapping from these different vector spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mappings(vocab,forward_map):\n",
    "    \"\"\"\n",
    "    This function takes the words in the vocabulary and creates a unique mapping to a number.\n",
    "    :param vocab contains all the words in the corpus.\n",
    "    :param forward_map a dictionary that will be populated with mappings.\n",
    "    returns populated forward_map\n",
    "    \"\"\"\n",
    "    for sample in vocab:\n",
    "        if sample not in forward_map.keys():\n",
    "            forward_map[sample]= len(forward_map)\n",
    "            \n",
    "    return forward_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOS_token = 2\n",
    "# EOS_token = 3\n",
    "word2index = {'<PAD>': 0, '<UNK>':1,'<SOS>':2,'<EOS>':3}\n",
    "create_mappings(atis_vocab,word2index)\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index = {'<PAD>' : 0,'<UNK>':1,'<SOS>':2,'<EOS>':3}\n",
    "create_mappings(atis_slots,tag2index)\n",
    "index2tag = {v:k for k,v in tag2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent2index={}\n",
    "create_mappings(atis_intents,intent2index)\n",
    "index2intent = {v:k for k,v in intent2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we create a Tensor where each row is a mapped/embedded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq_data, mapping,map_type):\n",
    "    \"\"\"\n",
    "    :param seq a sequnce which will be embedded as a vector\n",
    "    :param mapping, a dictionary which contains how each element in the seq will be mapped to a number.\n",
    "    :param map_type 'words','slots' or 'intent'\n",
    "    returns a Pytorch Tensor.\n",
    "    \"\"\"\n",
    "    if map_type=='intent':\n",
    "        intent = seq_data[map_type]\n",
    "        embeddings = mapping[intent] if intent in mapping.keys() else -1 #mapping[\"<UNK>\"]\n",
    "        return torch.tensor(embeddings)   \n",
    "    else:\n",
    "        embed_fnc = lambda word: mapping[word] if word in mapping.keys() else mapping[\"<UNK>\"]\n",
    "        embeddings = list(map(embed_fnc, seq_data[map_type])) \n",
    "        return torch.LongTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(padded_atis):\n",
    "    \"\"\"\n",
    "    :param padded_atis, this is padded sequence data.\n",
    "           Of the form seq,slots,intent. This function coverts \n",
    "           these into tensors.\n",
    "    return train_data; [(seq_tensor,slot_tensor,intent_tensor)]\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    atis_data = padded_atis['data']\n",
    "    for i in range(len(atis_data)):\n",
    "        seq_tensor = prepare_sequence(atis_data[i],word2index,'words')\n",
    "        slot_tensor = prepare_sequence(atis_data[i],tag2index,'slots')\n",
    "        intent_tensor = prepare_sequence(atis_data[i],intent2index,'intent')\n",
    "        train_data.append((seq_tensor,slot_tensor,intent_tensor))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_training_set(adjusted_atis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_batch(batch):\n",
    "#     print(len(batch), len(batch[0]))\n",
    "    #     print([ex[0] for ex in batch])\n",
    "    seqs = torch.stack([ex[0] for ex in batch])\n",
    "    slots = torch.stack([ex[1] for ex in batch])\n",
    "    intents = torch.stack([ex[2] for ex in batch])\n",
    "   \n",
    "    return seqs,slots,intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size, train_data):\n",
    "    \"\"\"\n",
    "    Returns iteratively a batch of specified size on the data. \n",
    "    The last batch can be smaller if the total size is not multiple of the batch\n",
    "    \"\"\"\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while sindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex] #list of batch_size num of tuples.\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        #print('returning', len(batch), 'samples')\n",
    "        yield concatenate_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches = get_batches(5,train_data[:10])\n",
    "# list(batches)[0][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Joint -RNNless Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An encoder/decoder that\n",
    "    takes a batch of sequences embeds the sequence and\n",
    "    then runs it through a fully connected layer to predict slots and intent.\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim,slot_dim,intent_dim,emb_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slot_dim = slot_dim\n",
    "        self.intent_dim = intent_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.slot_fc = nn.Linear(emb_dim,slot_dim)\n",
    "        self.intent_fc = nn.Linear(emb_dim,intent_dim)\n",
    "        \n",
    "    def forward(self,src):\n",
    "        embedded = self.embedding(src)# get embedding for a word\n",
    "        #print(embedded.size())\n",
    "        slots = self.slot_fc(embedded)# then predict a slot using fc\n",
    "        \n",
    "        # Need average embedding for a sequence\n",
    "        avg_embedding = embedded.mean(dim=1) #[50,5]-> [5] because don't have 50 different elements anymore just one.\n",
    "        #print('embed',embedded.size())\n",
    "        #print('avg embed:',avg_embedding.size())\n",
    "        intent = self.intent_fc(avg_embedding)\n",
    "        #print('intent pred',intent.size())\n",
    "        # then put average embedding into intent fc\n",
    "        \n",
    "        return slots,intent,embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing models...\")\n",
    "n_layers = 1\n",
    "INPUT_DIM = len(word2index)\n",
    "SLOT_DIM = len(tag2index)\n",
    "print('Inp: ',INPUT_DIM,'Slotdim: ',SLOT_DIM)\n",
    "INTENT_DIM = len(index2intent)\n",
    "embed_size = 5\n",
    "inp = get_batches(1,train_data[:1])\n",
    "encoder = Encoder(INPUT_DIM,SLOT_DIM,INTENT_DIM,embed_size)\n",
    "# testing avg embedding\n",
    "for data in inp:\n",
    "    input,label,real_intent = data[0],data[1],data[2]#.type(torch.FloatTensor)\n",
    "    slot,intent,_ = encoder(input)\n",
    "    print(_.size())\n",
    "    slot = torch.argmax(slot,dim=2)\n",
    "    test_criterion = nn.CrossEntropyLoss()\n",
    "#     print(real_intent.size(),intent.size())\n",
    "#     print('real intent',real_intent,'predicted intent',intent)\n",
    "    test_criterion(intent,real_intent)\n",
    "    #intent = torch.argmax(intent,dim=1).type(torch.FloatTensor)\n",
    "    #print(slot.size())\n",
    "    #print(intent.size())\n",
    "#batches = get_batches(5,train_data[:10]) # to get each representation of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_criterion = nn.CrossEntropyLoss()\n",
    "real_intent,intent\n",
    "test_criterion(intent,real_intent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model & Running Experiments\n",
    "Here we vary the embedding size in the embedding layer while keeping the learning rate, batch_size, optimizer \n",
    "and other network parameters constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,num_epochs,batch_size,optimizer,train_data,constant_params):\n",
    "    loss_by_epoch = {}\n",
    "    criterion = constant_params['criterion']\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        slot_losses = []\n",
    "        intent_losses = []\n",
    "        inp = get_batches(batch_size,train_data)\n",
    "        for data in inp:\n",
    "            inputs, labels, intents = data[0],data[1],data[2]\n",
    "            inputs = inputs.type(torch.LongTensor).to(device)\n",
    "            labels = labels.type(torch.LongTensor).view(-1).to(device) #squashing\n",
    "            intents = intents.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            out_slots, out_intents,_ = model(inputs)\n",
    "            out_slots = out_slots.view(-1,constant_params['SLOT_DIM'])\n",
    "\n",
    "            slot_loss = criterion(out_slots, labels)\n",
    "            intent_loss = criterion(out_intents, intents) \n",
    "            slot_losses.append(slot_loss.item())\n",
    "            intent_losses.append(intent_loss.item())\n",
    "            \n",
    "            slot_loss.backward(retain_graph=True)\n",
    "            intent_loss.backward()\n",
    "            optimizer.step()\n",
    "        slot_col_name = \"epoch:\"+str(epoch+1)+\"_slot_losses\"\n",
    "        intent_col_name = \"epoch:\"+str(epoch+1)+\"_intent_losses\"\n",
    "        loss_by_epoch[slot_col_name] = slot_losses\n",
    "        loss_by_epoch[intent_col_name] = intent_losses\n",
    "        \n",
    "    return loss_by_epoch #slot_losses, intent_losses,\n",
    "\n",
    "def make_models(constant_params,vary_params):\n",
    "    #make a model with a different embedding size\n",
    "    model_list = []\n",
    "    for i in range(len(vary_params['embed_size'])):\n",
    "        model_list.append( Encoder(constant_params['INPUT_DIM'],\n",
    "                                      constant_params['SLOT_DIM'],\n",
    "                                      constant_params['INTENT_DIM'],\n",
    "                                      vary_params['embed_size'][i]).to(device) )\n",
    "    return model_list\n",
    "\n",
    "def run_experiment(constant_params,vary_params,num_runs,exp_data,model_type='ann'):\n",
    "    experiment_results = {}\n",
    "    list_of_dfs = []\n",
    "    for run in range(num_runs): # fill dict with keys\n",
    "        for i in range(len(vary_params['embed_size'])):\n",
    "            col_name = 'run_'+str(run+1)+'_embed_size_'+str(vary_params['embed_size'][i])\n",
    "            experiment_results[col_name] = []\n",
    "           \n",
    "    for run in range(num_runs):# run actual experiments\n",
    "        run_model_list = make_models(constant_params,vary_params)\n",
    "        for i in range(len(run_model_list)):\n",
    "            optimizer = optim.Adam(run_model_list[i].parameters(), lr=0.01)\n",
    "            col_name = 'run_'+str(run+1)+'_embed_size_'+str(vary_params['embed_size'][i])\n",
    "           \n",
    "            loss_by_epoch = train_model(run_model_list[i],\n",
    "                                                    constant_params['num_epoch'],\n",
    "                                                   constant_params['batch_size'],\n",
    "                                                   optimizer,\n",
    "                                                   exp_data,\n",
    "                                                   constant_params)\n",
    "            \n",
    "            result_dataframe = pd.DataFrame(loss_by_epoch,index=None)\n",
    "            result_dataframe.to_csv(\"experiment_results/\"+\n",
    "                                    str(model_type)+\n",
    "                                    str(run)+\"_\"+\n",
    "                                    str(vary_params['embed_size'][i])+\"_\"\n",
    "                                    \"train_experiment.csv\")\n",
    "            list_of_dfs.append(result_dataframe)\n",
    "            #experiment_results[col_name].append(loss_by_epoch)\n",
    "            #experiment_results[col_name].append(run_intent_losses)\n",
    "                \n",
    "    # after experiments are over save as a .csv\n",
    "#     result_dataframe = pd.DataFrame(experiment_results,index=None)\n",
    "#     result_dataframe.to_csv(\"experiment_results/\"+str(model_type)+\"train_experiments.csv\")\n",
    "    return list_of_dfs #incase we want it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_exp_params = {'batch_size':128,\n",
    "                       'num_epoch':200,\n",
    "                      'INPUT_DIM':len(word2index),\n",
    "                      'SLOT_DIM':len(tag2index),\n",
    "                      'INTENT_DIM':len(index2intent),\n",
    "                      'criterion': nn.CrossEntropyLoss(ignore_index=0,reduction='elementwise_mean')}\n",
    "\n",
    "variable_exp_params = {'embed_size':[2,4,8,16,32,64,128,256]}\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "run_experiment(constant_exp_params,variable_exp_params,5,train_data,model_type='ann')\n",
    "end = time.time()\n",
    "elapsed = (end-start)/60.\n",
    "print('Time elapsed: %.4f mins ' % (elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "max_length = 50\n",
    "INPUT_DIM = len(word2index)#\n",
    "SLOT_DIM = len(tag2index)\n",
    "INTENT_DIM = len(index2intent)\n",
    "embed_size = 10 # tried 5\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM,SLOT_DIM,INTENT_DIM,embed_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0,reduction='elementwise_mean')#ignore <pad>\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(atis_intents)\n",
    "# len(atis_slots)\n",
    "# atis_slots\n",
    "# tag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    inp = get_batches(batch_size,train_data)\n",
    "    encoder.train()\n",
    "    for data in inp:\n",
    "        # get the inputs\n",
    "        inputs, labels, intents = data[0],data[1],data[2]\n",
    "        inputs = inputs.type(torch.LongTensor).to(device)\n",
    "        labels = labels.type(torch.LongTensor).view(-1).to(device) #squashing\n",
    "        intents = intents.to(device)#.type(torch.FloatTensor)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        out_slots, out_intents,_ = encoder(inputs)\n",
    "        out_slots = out_slots.view(-1,SLOT_DIM)\n",
    "        \n",
    "        slot_loss = criterion(out_slots, labels)\n",
    "        intent_loss = criterion(out_intents, intents) \n",
    "        \n",
    "        slot_loss.backward(retain_graph=True)\n",
    "        intent_loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%25==0:\n",
    "        print('Slot Loss after epoch '+str(epoch)+':',slot_loss.item())\n",
    "        print('Intent Loss after epoch '+str(epoch)+':',intent_loss.item())\n",
    "        print('-----------------------------------------------')\n",
    "end = time.time()\n",
    "elapsed = (end-start)/60.\n",
    "print('Time elapsed: %.4f mins ' % (elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Encoder\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed/fold_test.json', 'r') as f:\n",
    "    atis_test_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(atis_test_json['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atis_test_json['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_atis_test = adjust_sequences(atis_test_json)#padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocab,test_slots,test_intents = get_vocabularies(adjusted_atis_test)\n",
    "#test_intents,index2intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_training_set(adjusted_atis_test) # should we cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)#[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude pad \n",
    "def calc_bat_fscores(y_pred,y_true,predict_type='slot'):\n",
    "    \"\"\"\n",
    "    Calc average f score for a batch.\n",
    "    compares each predicted output in a batch to actual output.\n",
    "    Then averages that.\n",
    "    \"\"\"\n",
    "    batch_avg_f = []\n",
    "    keep_slots = [i for i in tag2index.values() if i > 2]\n",
    "    if predict_type=='slot':\n",
    "        for pred_i in range(len(y_pred)):# range batch size\n",
    "            #print(y_pred[pred_i].size())\n",
    "            f_score = f1_score(y_true[pred_i], y_pred[pred_i],labels=keep_slots, average ='micro')  \n",
    "            batch_avg_f.append(f_score)\n",
    "    else:\n",
    "        return f1_score(y_true,y_pred,average ='micro')\n",
    "    return np.mean(batch_avg_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')# get a lot of warnings because some labels are not predicted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = get_batches(256,test_data)\n",
    "encoder.eval()\n",
    "f_slot_scores = []\n",
    "f_intent_scores = []\n",
    "for data in inp:\n",
    "    # get the inputs\n",
    "    inputs, labels , true_intents = data[0],data[1],data[2]\n",
    "    inputs = inputs.type(torch.LongTensor).to(device)\n",
    "    labels = labels.to(device)\n",
    "    true_intents = true_intents.to(device)\n",
    "    \n",
    "#     if USE_CUDA and torch.cuda.is_available():\n",
    "#         inputs = inputs.cuda()\n",
    "#         labels = labels.cuda()\n",
    "    \n",
    "    slots ,intents, slot_embedding = encoder(inputs)\n",
    "    y_pred_slots = torch.argmax(slots,dim=2)\n",
    "    f_slot_scores.append(calc_bat_fscores(y_pred_slots,labels))\n",
    "    y_pred_intents = torch.argmax(intents,dim=1)\n",
    "    #print(y_pred_intents)\n",
    "    #print(true_intents)\n",
    "    #print(list(zip(true_intents.tolist(), y_pred_intents.tolist())))\n",
    "    f_intent_scores.append(calc_bat_fscores(y_pred_intents,true_intents,predict_type='intent'))\n",
    "print('Mean Slot F Metric :',np.mean(f_slot_scores))\n",
    "print('-----------------------------------------------')\n",
    "print('Mean Intent F Metric :',np.mean(f_intent_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # Change line width\n",
    "# plt.xlabel('Slot F1-Scores')\n",
    "# sns.boxplot(y = f_slot_scores, linewidth=1,width=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix For Intents since there are much less "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    category_i = output.item()\n",
    "    return index2intent[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in get_batches(1,test_data[0:1]):\n",
    "    input,_,intent = data[0],data[1],data[2]\n",
    "    input = input.to(device)\n",
    "    #print(input.size())\n",
    "    output,intent_pred,_  = encoder(input)\n",
    "    predictions = torch.argmax(intent_pred,dim=1)[0]\n",
    "    print(output.size())\n",
    "    print(predictions)\n",
    "    print(categoryFromOutput(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = []\n",
    "intent_labels = ['abbreviation','aircraft','aircraft&flight&flight_no',\n",
    "                 'airfare','airline','airline_flight_no','airport','capacity',\n",
    "                'cheapest','city','distance','flight','flight&airfare','flight_no','flight_time',\n",
    "                 'ground_fare','ground_fare_service','ground_service&ground_fare','meal',\n",
    "                'quantity','restriction']\n",
    "for category in intent2index.values():\n",
    "    all_categories.append(category)\n",
    "      \n",
    "len(all_categories),len(intent_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intent_labels\n",
    "#index2intent\n",
    "#len(intent_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png','pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "n_categories = len(all_categories)\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 5000\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for data in get_batches(1,train_data):\n",
    "    inputs, labels , true_intents = data[0],data[1],data[2]\n",
    "    inputs = inputs.type(torch.LongTensor).to(device)\n",
    "    labels = labels.to(device)\n",
    "    true_intents = true_intents.to(device)\n",
    "    output,intent_pred,_  = encoder(inputs)\n",
    "    predictions = torch.argmax(intent_pred,dim=1)[0]\n",
    "    #print(predictions, true_intents)\n",
    "    guess, guess_i = categoryFromOutput(predictions)\n",
    "    if true_intents.item()!= -1:#intent is in dataset\n",
    "        category_i = all_categories.index(true_intents)\n",
    "        confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + intent_labels, rotation=90)\n",
    "ax.set_yticklabels([''] + intent_labels) #true\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"nn_cm_with_intent\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"foo.pdf\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.savefig(\"foo.pdf\")\n",
    "f.savefig(\"foo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE\n",
    "* Starting with 100 samples from the test set\n",
    "* Then keeping track of the labels we pass the samples through our model to get an embedding: [batch size,50,5]\n",
    "* With this embedding we try to drop remove the embeddings of 0 since they will appear often. \n",
    "* With the 0s removed, we apply T-SNE. \n",
    "* Plot the T-SNE embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Would a t-SNE visualization be useful?\n",
    "Filter out top ten popular classes in testing set.\n",
    "Then randomly sample slot emebddings from those.\n",
    "Do the t-SNE embedding.\n",
    "Remember embeddings are in order, this helps to label.\n",
    "\n",
    "Looking t-SNE to visualize the results? How do the intent predictions and querie predictions look.\n",
    "Are the intents near the queries when visualized?\n",
    "\n",
    "Put in embedding into t-SNE and hopefully observe spatial semantic labels\n",
    "\n",
    "output should be 870,2\n",
    "\n",
    "color according to slot value and hopefully see color clusters\n",
    "\n",
    "## COLOR BY INTENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = get_batches(10,test_data)\n",
    "encoder.eval()\n",
    "for data in inp:\n",
    "    inputs, labels , true_intents = data[0],data[1],data[2]\n",
    "    inputs = inputs.type(torch.LongTensor).to(device)\n",
    "    slots ,intents, slot_embedding = encoder(inputs)\n",
    "slot_embedding.size()# the way it is now this is embedding of inp[90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_as_vector = slot_embedding[0][-1].cpu().detach().numpy()# will always be a pad\n",
    "pad_as_vector#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slot_embedding#.size()\n",
    "reshaped_slot_embed = slot_embedding.reshape(-1,embed_size).cpu().detach().numpy()\n",
    "#reshaped_slot_embed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/26154711/filter-rows-of-a-numpy-array/26154854\n",
    "def remove_zero_embed(row,zero_embed):\n",
    "    return row != zero_embed\n",
    "bool_arr = np.array([remove_zero_embed(row,pad_as_vector) for row in reshaped_slot_embed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_slot_wo_zero = reshaped_slot_embed[bool_arr].reshape(-1,embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_slot_wo_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = TSNE(n_components=2,init='pca',random_state=423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = tsne.fit_transform(reshaped_slot_wo_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "reshaped_labels = labels.reshape(-1).detach().numpy()\n",
    "reshaped_labels_wo_zero = reshaped_labels[reshaped_labels>0]\n",
    "reshaped_labels_wo_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_tsne[:,0])\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "for i in range(X_tsne.shape[0]):\n",
    "#     dist = np.sum((X_tsne[i] - shown_images) ** 2, 1)\n",
    "#             if np.min(dist) < 4e-3:\n",
    "#                 # don't show points that are too close\n",
    "#                 continue\n",
    "    plt.text(X_tsne[i, 0], X_tsne[i, 1], str(reshaped_labels_wo_zero[i]),\n",
    "             color=plt.cm.Set1(reshaped_labels_wo_zero[i] / 100.),\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.scatter(X_tsne[:,0],X_tsne[:,1])\n",
    "plt.title('t-SNE visualization of test data')\n",
    "fig = ax.get_figure()\n",
    "#fig.savefig(\"tsne\",dpi=300)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
